Save Scripts: Save the code above as train_rl_cco.py and predict_cco_config.py in your traffic directory.
(Recommended) Move the CCO_RL_Env class definition into its own file (e.g., cco_rl_env.py) and import it into train_rl_cco.py.
Install Library: pip install stable-baselines3[extra] tensorflow or pip install stable-baselines3[extra] torch (depending on your preferred backend). Add tensorboard if you want to use it: pip install tensorboard.
Prepare Inputs for Training:
Make sure your pre-trained backend RF model (BAYESIAN_DIGITAL_TWIN_MODEL_ID) exists and is correct.
Ensure topology.csv is present in ./data/.
Ensure the ./ue_data/ directory exists and contains generated_ue_data_for_cco_0.csv through generated_ue_data_for_cco_23.csv (generated by traffic_3.py).
Tune REWARD_WEIGHTS in train_rl_cco.py - this is crucial and requires experimentation.
Adjust TOTAL_TRAINING_TIMESTEPS as needed. Start smaller (e.g., 50000) to test, then increase for full training.
Train: Run python train_rl_cco.py. Monitor the output and TensorBoard logs (run tensorboard --logdir ./rl_logs/ in a separate terminal). Training will take time due to the backend simulations.
Predict: Once training is done and cco_rl_agent_ppo.zip (or similar) is saved:
Run python predict_cco_config.py --tick <hour> (replace <hour> with 0-23).
Example: python predict_cco_config.py --tick 14
It will load the model and print the predicted optimal cell_el_deg for each cell_id for that specific hour. You can adjust the model path (-m) and topology path (-t) if needed.
Remember this is a complex setup. The reward function tuning and RL hyperparameters often require significant experimentation to get good results.